## MultiLayerPerceptron
#An implementation of Multi Layer Perceptron for MNIST dataset

This project is an basic implementation of multi layer perceptron without using any ML libraries.
The base idea goes parallel to Simon Haykin's Neural Networks: A Comprehensice Foundation book - chapter 4 - Multilayer Perceptrons.
Proposed network learns with back propagation and uses only sigmoid activation function. 

Different learning rates and proper momentum rates are used for each layer. 
Each layer's weighth initiated within the range -1 and 1 with 0.1 margin. 

